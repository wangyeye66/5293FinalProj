---
title: "EDA"
author: "Han Wang"
date: "2023-04-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r}
library(readxl)
library(tidyverse)
library(lubridate)

data <- read_excel('online_retail.xlsx')
summary(data)
```

```{r}
data %>% is.na %>% colSums
```

There are many NAs in the column CustomerID. Since this column is irrelavent to our analysis, we can simply remove it.

### Data Cleaning

```{r}
# remove free orders
data <- data %>%
  filter(UnitPrice != 0)

# remove column CustomerID
data <- data %>% select(-c(CustomerID))
summary(data)
```

### Data pre-processing

```{r}
# Transform description and country as factor
data <- data %>% mutate(Description = as.factor(Description),
                        Country = as.factor(Country))

# datetime processing
data$date <- format(data$InvoiceDate, "%m/%d/%Y")
data$month <- format(data$InvoiceDate, "%B")
data$week <- format(data$InvoiceDate, "%A")
data$time <- format(data$InvoiceDate, "%H")

# sales
data$Sales <- data$Quantity*data$UnitPrice

head(data)
```

### Exploratory Data Analysis

1. Most sold Products

```{r}
# top 10 products
p1 <- data %>% group_by(Description, StockCode) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10)

# plot
p1 %>% ggplot(aes(x=Description, y=count)) +
  geom_bar(stat = 'identity', fill = 'darkblue')+
  coord_flip() + theme_classic()+
  xlab('Products') +
  ylab('Number of products sold') +
  ggtitle('Most Sold Products by Customers')
```

2. Costomer base across the countries

```{r}
ggplot(data=data, aes(x=Country))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
  scale_y_continuous(labels = scales::format_format(scientific = FALSE))+
  ggtitle("UK shares the major customer base")+
  ylab("frequency based on orders")
```
Since almost all of users are from United Kingdom, it would be almost impossible to predict sales using country factor. Thus, we are going to drop the country column for this project.

3. Total Sales by Hour

Note that the original dataset does not include sales information from 9 p.m. to 6 a.m. It is possible that this is due to website maintenance, or someone may have excluded this data due to the low number of sales during those hours.

```{r}
# calculate total sales in hours
hourly <- data %>%
  group_by(time) %>%
  summarise(Sales = sum(Sales))

# line plot
ggplot(data = hourly, aes(x= time, y = Sales, group =1))+
  ggtitle('Total Sales by Hour')+
  geom_line(colour = 'orange', size= 1.2) +
  geom_point(color = 'darkgreen')+
  theme(plot.title = element_text(hjust = 0.5, lineheight = 0.8, face = 'bold')) +
  ylab('Total sales')+
  xlab('Hours')

```

4. Total Sales by Day

AS mentioned before, the dataset does not include sales information of Saturday due to similar reasons.

```{r}
# calculate total sales on daily basis
daily <- data%>%
  group_by(week)%>%
  summarise(Sales = sum(Sales)) %>%
  arrange(week)

daily$week <- factor(daily$week, levels =
              c("Sunday","Monday","Tuesday", "Wednesday", "Thursday",   "Friday"))
daily <- daily[order(daily$week),]

#line plot
ggplot(data = daily, aes(x= week, y = Sales, group = 1))+
  ggtitle('Total Sales by Day') +
  geom_line(color ='skyblue',size = 1.2)+
  geom_point(color = 'darkblue')+
  theme(plot.title =element_text(hjust = 0.5, lineheight = 0.8, face = 'bold'))+
  ylab('Total Sales') +
  xlab('Days of the week')

```

5. Total Sales by Month

```{r}
# calculating the total sales by month 
p5 <- data %>%
  group_by(month)%>%
  summarise(Sales=sum(Sales))%>%
  arrange(match(month, month.name))

#Line plot
ggplot(data=p5,aes(x=ordered(month, month.name), y=Sales, group =1))+ggtitle("Total Sales by Month ")+
  geom_line(color = "maroon", size = 1.2)+
  geom_point(color="darkblue")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),plot.title = element_text(hjust=0.5, lineheight = 0.8, face = "bold"))+
  ylab("Total Sales") + xlab("Months of the Year")

```

### Check for Multicollinearity
```{r}
mydata <- subset(data, select = -c(InvoiceNo, StockCode, Description, InvoiceDate,date, Country))
mydata$month <-recode(mydata$month,December = 12, November =11, October = 10,
                      September=9, August = 8, July = 7, June = 6, 
                      May = 5, April = 4, March = 3, February = 2, January =1)
mydata$week <-recode(mydata$week, Monday = 1, Tuesday =2, Wednesday =3,
                     Thursday =4, Friday =5, Sunday =7)
mydata$time <- as.numeric(mydata$time)
cor(subset(mydata, select = -Sales))
```

All of dependent variables are not correlated to each other, so we will keep these factors.

### Checking for Principal Components
```{r}
data.pca <- prcomp(subset(mydata, select = -Sales), center = T,scale = T)
summary(data.pca)

var <- data.pca$sdev^2
pve <- var/sum(var)

g<-qplot(x = 1:5, y = cumsum(pve), geom = 'line',
         xlab = 'Number of Principal Components',
         ylab = 'Proportion of Variance Explained',
         main = 'Scree Plot of Proportion of Variance Explained')

g + scale_x_continuous(breaks = seq(0, 5, by = 1))
```
Almost a linear curve, that means we have good choice of dependent variables.

### save cleaned data

The original dataset has around 540,000 pieces of data, to make it computationally efficient, I randomly selected 50,000 data.
```{r}
mydata <- subset(data, select = -c(InvoiceNo, StockCode, Description, InvoiceDate,date, Country))
mydata.sample <- mydata[sample(nrow(mydata), "50000"), ]

write.csv(mydata, "./online_retail_cleaned.csv", row.names=TRUE)
```
