---
title: "IMLV Final Project"
author: "Han Wang"
date:  "`r Sys.Date()`"
output: bookdown::bookdown_site
---

## Read data from Excel file

After loading the data, use `summary()` to get an overview of it.

```{r}
library(readxl)
library(tidyverse)
library(lubridate)

data <- read_excel('online_retail.xlsx')
summary(data)
```

```{r}
data %>% is.na %>% colSums
```

There are many NAs in the column CustomerID. Since this column is irrelavent to our analysis, we can simply remove it.

## Data Cleaning

Remove `NULL` values from the data, and reprint the summary of the data.

```{r}
# remove free orders
data <- data %>%
  filter(UnitPrice != 0)

# remove column CustomerID
data <- data %>% select(-c(CustomerID))
summary(data)
```

## Data pre-processing

I changed the `description` and `country` column to factor, and did the datetime processing. In particular, I extracted the column `InvoiceDate`, separating out four new columns: `date`, `month`, `week`, and `time`. The `Sales` column is then calculated using `Quantity` and `UnitPrice`.

```{r}
# Transform description and country as factor
data <- data %>% mutate(Description = as.factor(Description),
                        Country = as.factor(Country))

# datetime processing
data$date <- format(data$InvoiceDate, "%m/%d/%Y")
data$month <- format(data$InvoiceDate, "%B")
data$week <- format(data$InvoiceDate, "%A")
data$time <- format(data$InvoiceDate, "%H")

# sales
data$Sales <- data$Quantity*data$UnitPrice

head(data)
```

## Exploratory Data Analysis

### Most sold Products

```{r}
# top 10 products
p1 <- data %>% group_by(Description, StockCode) %>%
  summarise(count = n()) %>%
  arrange(desc(count)) %>%
  head(10)

# plot
p1 %>% ggplot(aes(x=Description, y=count)) +
  geom_bar(stat = 'identity', fill = 'darkblue')+
  coord_flip() + theme_classic()+
  xlab('Products') +
  ylab('Number of products sold') +
  ggtitle('Most Sold Products by Customers')
```
The bar plot above depicts the top ten most popular products on this online retail website. According to the plot, the most popular product is the 'White Hanging Heart T-Light Holder,' with approximately 2500 units sold. The products 'Regency Cakestand 3 Tier' and 'Jumbo Bag Red Retrospot' are also popular on the online retail website.

### Costomer base across the countries

```{r}
ggplot(data=data, aes(x=Country))+
  geom_bar()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))+
  scale_y_continuous(labels = scales::format_format(scientific = FALSE))+
  ggtitle("UK shares the major customer base")+
  ylab("frequency based on orders")
```

Given that almost all users are from the United Kingdom, it would be nearly impossible to predict sales using the country factor. As a result, we will remove the `country` column from this project.

### Total Sales by Hour

Note that the original dataset does not include sales information from 9 p.m. to 6 a.m. It is possible that this is due to website maintenance, or someone may have excluded this data due to the low number of sales during those hours.

```{r}
# calculate total sales in hours
hourly <- data %>%
  group_by(time) %>%
  summarise(Sales = sum(Sales))

# line plot
ggplot(data = hourly, aes(x= time, y = Sales, group =1))+
  ggtitle('Total Sales by Hour')+
  geom_line(colour = 'orange', size= 1.2) +
  geom_point(color = 'darkgreen')+
  theme(plot.title = element_text(hjust = 0.5, lineheight = 0.8, face = 'bold')) +
  ylab('Total sales')+
  xlab('Hours')

```

The line graph depicts total sales by hour. The majority of sales occur between 10 a.m. and 3 p.m. This makes logical because people are more active during this time period. It's worth noting that this line plot is quite symmetric. People are less likely to buy in the early morning and late at night.

### Total Sales by Day

AS mentioned before, the dataset does not include sales information of Saturday due to similar reasons.

```{r}
# calculate total sales on daily basis
daily <- data%>%
  group_by(week)%>%
  summarise(Sales = sum(Sales)) %>%
  arrange(week)

daily$week <- factor(daily$week, levels =
              c("Sunday","Monday","Tuesday", "Wednesday", "Thursday",   "Friday"))
daily <- daily[order(daily$week),]

#line plot
ggplot(data = daily, aes(x= week, y = Sales, group = 1))+
  ggtitle('Total Sales by Day') +
  geom_line(color ='skyblue',size = 1.2)+
  geom_point(color = 'darkblue')+
  theme(plot.title =element_text(hjust = 0.5, lineheight = 0.8, face = 'bold'))+
  ylab('Total Sales') +
  xlab('Days of the week')

```
The line plot Total Sales by Day depicts costomers' shopping habit. Sales are higher form Monday to Thursday, while sales drops significantly on Friday and weekends.

### Total Sales by Month

```{r}
# calculating the total sales by month 
p5 <- data %>%
  group_by(month)%>%
  summarise(Sales=sum(Sales))%>%
  arrange(match(month, month.name))

#Line plot
ggplot(data=p5,aes(x=ordered(month, month.name), y=Sales, group =1))+ggtitle("Total Sales by Month ")+
  geom_line(color = "maroon", size = 1.2)+
  geom_point(color="darkblue")+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),plot.title = element_text(hjust=0.5, lineheight = 0.8, face = "bold"))+
  ylab("Total Sales") + xlab("Months of the Year")

```

This line plot depicts the month-to-month change in online retail sales. The figure shows that sales are high from September to December. The peak month is November. This figure makes sense because there are numerous festivities from September through December, such as Christmas, Thanksgiving, and New Year's. We can deduce that people shop for gifts online. Another trend in the figure is a significant dip in January, and total sales for the remainder of the year are relatively low. It is most likely due to limited budgets and a lack of festivals from January to August.

## Change Categorical to Numerical

```{r}
mydata <- subset(data, select = -c(InvoiceNo, StockCode, Description, InvoiceDate,date, Country))
mydata$month <-recode(mydata$month,December = 12, November =11, October = 10,
                      September=9, August = 8, July = 7, June = 6, 
                      May = 5, April = 4, March = 3, February = 2, January =1)
mydata$week <-recode(mydata$week, Monday = 1, Tuesday =2, Wednesday =3,
                     Thursday =4, Friday =5, Sunday =7)
mydata$time <- as.numeric(mydata$time)
# Check for Multicollinearity
cor(subset(mydata, select = -Sales))
```

I converted categorical data to numerical data in this stage. For example, 'December' equals 12, 'Monday' equals 1, and so on. The variables are next checked for multicollinearity by calculating the correlations between them. Except for the diagnal, the values in the above table are very low. As a result, we concluded that all of the variables are unrelated, and we should keep them.

## Checking for Principal Components

```{r}
data.pca <- prcomp(subset(mydata, select = -Sales), center = T,scale = T)
summary(data.pca)

var <- data.pca$sdev^2
pve <- var/sum(var)

g<-qplot(x = 1:5, y = cumsum(pve), geom = 'line',
         xlab = 'Number of Principal Components',
         ylab = 'Proportion of Variance Explained',
         main = 'Scree Plot of Proportion of Variance Explained')

g + scale_x_continuous(breaks = seq(0, 5, by = 1))
```

The Scree plot is almost a linear curve, that means we have good choice of dependent variables.

## Drop Outliers

```{r}
q <- quantile(mydata$Sales, probs= c(.25, .75))
iqr <- IQR(mydata$Sales)
mydata.adj <- subset(mydata, mydata$Sales>(q[1]-1.5*iqr) & mydata$Sales < (q[2]+1.5*iqr))
```

When I was implementing the first model, I noticed that the original dataset has a large number of outliers. To improve prediction precision, I removed the first and fourth quantiles from the original data and then sampled 5000 data points from the modified data.

## Save Cleaned Data

The original dataset has around **540,000** pieces of data, to make it computationally efficient, I randomly selected **5000** data, and saved it as csv file.

```{r}
set.seed(111)
mydata.sample <- mydata[sample(nrow(mydata), "5000"), ]
set.seed(111)
mydata.sample.adj <- mydata.adj[sample(nrow(mydata.adj), "5000"), ]

write.csv(mydata.sample, "./online_retail_cleaned.csv", row.names=TRUE)
write.csv(mydata.sample.adj, "./online_retail_cleaned_adjusted.csv", row.names=TRUE)
```
